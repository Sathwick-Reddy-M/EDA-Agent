from typing import TypedDict, Annotated, List, Optional
from contextlib import asynccontextmanager
from pathlib import Path
import uuid

from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnableConfig

from langgraph.types import Send

from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver

from .helpers import (
    get_data_file_paths,
    answer_with_coding_agent,
    answer_with_plotting_agent,
)
from .data_models import QueryPlan

import os
from dotenv import load_dotenv

load_dotenv()

from utils.llm import get_llm, get_llm_with_structured_output
from utils.db import DB_PATH


def merge_dicts(left: dict[str, str], right: dict[str, str]) -> dict[str, str]:
    return {**left, **right}


class State(TypedDict):
    messages: Annotated[List[BaseMessage], add_messages]
    is_data_folder_empty: bool
    uploaded_files: dict[str, str]
    file_info_from_user: Annotated[dict[str, str], merge_dicts]
    file_info_from_llm: Annotated[dict[str, str], merge_dicts]
    query: str
    query_plan: QueryPlan
    coding_task: Optional[str]
    coding_task_output: Optional[str]
    plot_task: Optional[str]
    plot_task_output: Optional[str]
    is_coding_task_success: bool
    coding_task_error: Optional[str]
    is_plot_task_success: bool
    plot_task_error: Optional[str]
    final_answer: str


llm = get_llm("gemini-2.5-flash", "google_genai")

prompt = ChatPromptTemplate.from_messages(
    [
        MessagesPlaceholder(variable_name="system_message"),
        MessagesPlaceholder(variable_name="messages"),
    ]
)


async def check_for_the_files(state: State) -> State:
    """
    Check if the data folder is empty and update the state accordingly.
    """
    data_folder_path = Path(os.getenv("DATA_FOLDER_PATH"))

    if not data_folder_path:
        raise ValueError("DATA_FOLDER_PATH environment variable is not set.")
    uploaded_files = await get_data_file_paths(data_folder_path, ignore_hidden=True)

    return {
        "is_data_folder_empty": (len(uploaded_files) == 0),
        "uploaded_files": uploaded_files,
    }


def continue_from_check_for_the_files(state: State):
    if state["is_data_folder_empty"]:
        return END
    return [
        Send(
            "get_file_info_from_user",
            {"filename": f, "uploaded_files": state["uploaded_files"]},
        )
        for f in state["uploaded_files"]
    ] + [
        Send(
            "get_file_info_from_llm",
            {"filename": f, "uploaded_files": state["uploaded_files"]},
        )
        for f in state["uploaded_files"]
    ]


def get_file_info_from_user(state: State) -> str:
    filename = state["filename"]
    file_info_from_user = input(
        f"Provide a brief description of the file '{filename}': "
    )

    return {"file_info_from_user": {filename: file_info_from_user}}


async def get_file_info_from_llm(state: State, config: RunnableConfig) -> str:
    filename = state["filename"]

    task = f"I want to get a brief description of the file named '{filename}' which is stored at {state['uploaded_files'][filename]}. The description provided should include the basic details about the file such as number of rows, columns, data types of columns, unique values in each column, null value proportions. Finally, your understanding of what the file represents."

    file_info_from_llm, is_success = await answer_with_coding_agent(task, config)

    if not is_success:
        file_info_from_llm = f"Error getting file info from LLM: {file_info_from_llm}"

    return {"file_info_from_llm": {filename: file_info_from_llm}}


async def plan_next_steps_from_query(state: State, config: RunnableConfig) -> State:
    system_prompt = """
You are an expert data analyst. You will be given the following:
1. A user query. -- This is the question or task that the user wants to accomplish.
2. A list of files that have been uploaded by the user. Each file has a brief description provided by the user and/or generated by an LLM.

You have two tools at your disposal:
1. Coding Agent - This agent can generate Python code to analyze data, generate statistics, and literally any manipulation that can be done using Python code.
2. Plotting Agent - This agent can generate Python code to create visualizations using matplotlib, seaborn, or plotly. - If you want to create visualizations to help answer the user's query, use this agent.

Your task is to create a plan to divide the task into coding task and/or plotting task and should generate instructions on how to combine the results from both agents to answer the user's query. It's not necessary to use both agents. You can use either one of them or both of them based on the user's query.

If neither agent is required, then mention that in the instructions for merging the results and also include the answer to the user's query in the instructions.
     """

    user_prompt = f"""
Here is the query: {state['query']}.
Generate the following JSON object by breaking down the query into coding task and/or plotting task:
{QueryPlan.model_json_schema()}
    """

    query_plan = await (
        prompt | get_llm_with_structured_output(llm, QueryPlan)
    ).ainvoke(
        {
            "messages": [HumanMessage(user_prompt)],
            "system_message": [SystemMessage(system_prompt)],
        }
    )
    coding_task = (
        query_plan.coding_task
        + f"Here is the file info provided from the user {state['file_info_from_user']}. Here is generated file info from the llm {state['file_info_from_llm']}. Here are the file paths: {state['uploaded_files']}"
        if query_plan.is_coding_task_required
        else None
    )
    plot_task = (
        query_plan.plot_task
        + f"Here is the file info provided from the user {state['file_info_from_user']}. Here is generated file info from the llm {state['file_info_from_llm']}. Here are the file paths: {state['uploaded_files']}"
        if query_plan.is_coding_task_required
        else None if query_plan.is_plot_task_required else None
    )

    return {
        "messages": [
            SystemMessage(system_prompt),
            HumanMessage(user_prompt),
            AIMessage(query_plan.model_dump_json()),
        ],
        "query_plan": query_plan,
        "coding_task": coding_task,
        "plot_task": plot_task,
    }


def continue_from_plan(state: State):
    plan: QueryPlan = state["query_plan"]

    sends = []
    if plan.is_coding_task_required:
        sends.append(
            Send("get_help_from_coding_agent", {"coding_task": state["coding_task"]})
        )
    if plan.is_plot_task_required:
        sends.append(
            Send("get_help_from_plotting_agent", {"plot_task": state["plot_task"]})
        )
    return sends if sends else "combine_agent_results"


async def get_help_from_coding_agent(state: State, config: RunnableConfig) -> State:

    coding_task_output, is_success = await answer_with_coding_agent(
        task=state["coding_task"], config=config
    )

    return {
        "coding_task_output": coding_task_output,
        "is_coding_task_success": is_success,
        "coding_task_error": None if is_success else coding_task_output,
    }


async def get_help_from_plotting_agent(state: State, config: RunnableConfig) -> State:

    plot_task_output, is_success = await answer_with_plotting_agent(
        task=state["plot_task"], config=config
    )

    return {
        "plot_task_output": plot_task_output,
        "is_plot_task_success": is_success,
        "plot_task_error": None if is_success else plot_task_output,
    }


async def combine_agent_results(state: State, config: RunnableConfig) -> State:
    plan: QueryPlan = state.get("query_plan")

    sys = SystemMessage(
        content=(
            "You are a senior data analyst. Combine the provided analysis results and any plots related summaries "
            "into a concise, accurate answer for the user's query. "
            "If an agent reported an error, note it briefly and proceed with what is available."
        )
    )

    is_coding_required = plan.is_coding_task_required
    is_plotting_required = plan.is_plot_task_required

    coding_msg = (
        (
            f"The coding agent's was {'successful' if state['is_coding_task_success'] else 'unsuccessful'}.\n"
            f"Here is the {'coding task output' if state['is_coding_task_success'] else 'coding task error'}: {state['coding_task_output'] if state['is_coding_task_success'] else state['coding_task_error']}\n\n"
        )
        if is_coding_required
        else ""
    )

    plotting_msg = (
        (
            f"The plotting agent was {'successful' if state['is_plot_task_success'] else 'unsuccessful'}.\n"
            f"Here is the {'plot task output' if state['is_plot_task_success'] else 'plot task error'}: {state['plot_task_output'] if state['is_plot_task_success'] else state['plot_task_error']}\n\n"
        )
        if is_plotting_required
        else ""
    )

    human = HumanMessage(
        content=(
            f"Here is the user's original query: {state['query']}\n\n"
            f"{coding_msg}"
            f"{plotting_msg}"
            f"Here are the instructions to answer the query: {plan.instructions_for_merging_task_results}\n\n"
            "Based on the above, provide a final answer to the user's query."
        )
    )

    resp: AIMessage = await llm.ainvoke([sys, human], config=config)

    return {"messages": [resp.content], "final_answer": resp.content}


def create_eda_agent(checkpointer: AsyncSqliteSaver):
    """
    Create and return the compiled LangGraph agent.
    """
    graph = StateGraph(State)

    # First Require to upload atleast a single file or multiple files.

    graph.add_node("check_for_the_files", check_for_the_files)

    graph.add_edge(START, "check_for_the_files")

    graph.add_node("get_file_info_from_user", get_file_info_from_user)

    graph.add_conditional_edges(
        "check_for_the_files", continue_from_check_for_the_files
    )
    graph.add_edge("get_file_info_from_user", "plan_next_steps_from_query")
    graph.add_node("get_file_info_from_llm", get_file_info_from_llm)
    graph.add_edge("get_file_info_from_llm", "plan_next_steps_from_query")

    graph.add_node("plan_next_steps_from_query", plan_next_steps_from_query)
    graph.add_conditional_edges("plan_next_steps_from_query", continue_from_plan)
    graph.add_node("get_help_from_coding_agent", get_help_from_coding_agent)
    graph.add_node("get_help_from_plotting_agent", get_help_from_plotting_agent)

    graph.add_node("combine_agent_results", combine_agent_results)
    graph.add_edge("get_help_from_coding_agent", "combine_agent_results")
    graph.add_edge("get_help_from_plotting_agent", "combine_agent_results")
    graph.add_edge("combine_agent_results", END)

    return graph.compile(checkpointer=checkpointer)


@asynccontextmanager
async def get_compiled_graph():
    """
    Async context manager that yields a compiled graph with an AsyncSqliteSaver.

    Usage:
        async with get_compiled_graph() as app:
            ...
    """
    async with AsyncSqliteSaver.from_conn_string(str(DB_PATH)) as memory:
        app = create_eda_agent(memory)
        yield app
